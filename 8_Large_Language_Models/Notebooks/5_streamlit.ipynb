{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit\n",
        "Streamlit lets you transform your data scripts into interactive dashboards and prototypes in minutes, without needing front-end coding knowledge. This means you can easily share insights with colleagues, showcase your data science work, or even build simple machine learning tools, all within the familiar Python environment."
      ],
      "metadata": {
        "id": "W4lmzdFaqmgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1.&nbsp; Streamlit demo üöÄ\n",
        "We first need to install [streamlit](https://streamlit.io/) - as always, locally this is a one time thing, whereas on colab we need to do it each session.\n",
        "\n",
        "We're also installing [localtunnel](https://theboroer.github.io/localtunnel-www/) here, which allows us to run streamlit from colab. If you're working locally, you don't need to install this."
      ],
      "metadata": {
        "id": "g2MN3UoMhEdj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NGG2EFdgiPL",
        "outputId": "86b5d858-6bdc-4cfd-e180-24170917ca92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "+ off@0.0.10\n",
            "added 23 packages from 23 contributors and audited 23 packages in 2.425s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qqq streamlit --progress-bar off\n",
        "!npm install -qqq localtunnel --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of what you can produce with streamlit. It's so easy, just a few lines of python depending on what you want, and so many options!\n",
        "\n",
        "- Locally you would write this script in a .py file and not a notebook (.ipynb).\n",
        "\n",
        "- On colab, we can create a .py file by using the magic command `%%writefile` at the top of the cell. This command writes the cell content to a file, naming it 'app.py', or whatever else you choose, in this instance. Once saved, you can see 'app.py' in Colab's storage by clicking on the left-hand side folder icon."
      ],
      "metadata": {
        "id": "A6IbJEYVtNFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Title\n",
        "st.title(\"Streamlit Demo\")\n",
        "\n",
        "# Markdown\n",
        "st.markdown(\"\"\"\n",
        "This is a demo app showcasing a few of Streamlit's features.\n",
        "\n",
        "Streamlit is a powerful Python library for creating web apps. It is easy to use and has a wide range of features, including:\n",
        "\n",
        "* **Interactive widgets:** Streamlit makes it easy to create interactive widgets, such as sliders, dropdown menus, and radio buttons.\n",
        "* **Charts and graphs:** Streamlit can generate a variety of charts and graphs, including line charts, bar charts, and pie charts.\n",
        "* **Data display:** Streamlit can display data in a variety of ways, including tables, lists, and maps.\n",
        "* **Deployment:** Streamlit apps can be deployed to Heroku with a single command.\n",
        "\"\"\")\n",
        "\n",
        "# Slider\n",
        "slider_value = st.slider(\"Select a number:\", 0, 100)\n",
        "st.write(f\"You selected: {slider_value}\")\n",
        "\n",
        "# Dropdown menu\n",
        "dropdown_value = st.selectbox(\"Choose a color:\", [\"red\", \"green\", \"blue\"])\n",
        "st.write(f\"You chose: {dropdown_value}\")\n",
        "\n",
        "# Radio buttons\n",
        "radio_button_value = st.radio(\"Select a language:\", [\"English\", \"Spanish\", \"French\"])\n",
        "st.write(f\"You selected: {radio_button_value}\")\n",
        "\n",
        "# Text area\n",
        "text = st.text_area(\"Enter some text:\")\n",
        "if text:\n",
        "    st.write(f\"You entered: {text}\")\n",
        "\n",
        "# Button\n",
        "if st.button(\"Click me!\"):\n",
        "    st.write(\"You clicked the button!\")\n",
        "\n",
        "# Chart\n",
        "data = {\"x\": [1, 2, 3, 4, 5], \"y\": [6, 7, 2, 4, 5]}\n",
        "st.line_chart(data)\n",
        "\n",
        "# Map\n",
        "map_data = [\n",
        "    {\"name\": \"New York\", \"lat\": 40.7128, \"lon\": -74.0060},\n",
        "    {\"name\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437},\n",
        "    {\"name\": \"Chicago\", \"lat\": 41.8783, \"lon\": -87.6233},\n",
        "]\n",
        "st.map(map_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOXdinLGgriU",
        "outputId": "0d521ed9-f34c-4e1f-9eea-708a207ef5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run streamlit apps **locally**. Open the command line and navigate to the folder where you've stored the .py file. Then, use the command `streamlit run app.py`. If you used a different name for the .py file, change `app.py` for the name you used\n",
        "\n",
        "On **colab**, as we have to run streamlit through a tunnel. This is a little annoying for debugging, as every time you encounter a bug, you have to stop and reopen the tunnel. However, if you have a slower computer, or you simply wish to use Google's power so that your resources are free to do other things, it's very useful.\n",
        "\n",
        "When opening the tunnel it will ask you for a password. The password is your IP address, which will be printed out as the first element of the output from the code below."
      ],
      "metadata": {
        "id": "37HjErWnt9-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRcEI6cjgtrW",
        "outputId": "27bfd32f-0a43-4fd3-ed92-97ce152d9e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.34.23.251\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.704s\n",
            "your url is: https://easy-memes-repeat.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.&nbsp; RAG chatbot in streamlit ‚≠êÔ∏è\n",
        "We'll start by installing the same libraries and downloading the same files as in the previous notebooks."
      ],
      "metadata": {
        "id": "lUCdhQSihG5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rOEYKWwhI-d",
        "outputId": "702cc996-a5bf-4a10-82eb-9c477713f8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpuja86ou_\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:39<00:00, 111MB/s] \n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case you get the error 'NoneType' object has no attribute 'groups'\n",
        "# !pip install --upgrade gdown\n",
        "\n",
        "# download saved vector database for Alice's Adventures in Wonderland\n",
        "!gdown --folder 1A8A9lhcUXUKRrtCe7rckMlQtgmfLZRQH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okHR6MxMaLvU",
        "outputId": "e488c95f-f566-46d1-f3eb-1fb8d0bfb44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n",
            "Retrieving folder contents\n",
            "Processing file 1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt index.faiss\n",
            "Processing file 1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ index.pkl\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt\n",
            "To: /content/faiss_index/index.faiss\n",
            "100% 421k/421k [00:00<00:00, 47.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ\n",
            "To: /content/faiss_index/index.pkl\n",
            "100% 216k/216k [00:00<00:00, 102MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's proceed by creating the .py file for our rag chatbot.\n",
        "\n",
        "We sourced the foundational code for our Streamlit basic chatbot from the [Streamlit documentation](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).\n",
        "\n",
        "In addition, we implemented [cache_resource](https://docs.streamlit.io/library/api-reference/performance/st.cache_resource) for both memory and LLM. Given that Streamlit reruns the entire script with each message input, relying solely on memory would result in data overwriting and a loss of conversational continuity. The inclusion of cache resource prevents Streamlit from creating a new memory instance on each run. This was also added to the LLM, enhancing speed and preventing its reload in every iteration."
      ],
      "metadata": {
        "id": "ogWDZ5fbv7BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "@st.cache_resource\n",
        "def init_llm():\n",
        "  return LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "                  max_tokens = 2000,\n",
        "                  temperature = 0.1,\n",
        "                  top_p = 1,\n",
        "                  n_gpu_layers = -1,\n",
        "                  n_ctx = 1024)\n",
        "llm = init_llm()\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/embeddings\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "@st.cache_resource\n",
        "def init_memory(_llm):\n",
        "    return ConversationBufferMemory(\n",
        "        llm=llm,\n",
        "        output_key='answer',\n",
        "        memory_key='chat_history',\n",
        "        return_messages=True)\n",
        "memory = init_memory(llm)\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Chatier & chatier: conversations in Wonderland\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = chain(prompt)\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(answer[\"answer\"])\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kauMaNJ1hXNP",
        "outputId": "2d146aa5-787a-4e46-ec5a-70c58ee987e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing rag_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's load a tunnel and see what we've made"
      ],
      "metadata": {
        "id": "YqnlHR1Uv-88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run rag_app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQD-kDYbhZUV",
        "outputId": "5041413d-9eb7-4619-908f-803b263631ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.239.60.84\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.572s\n",
            "your url is: https://shy-masks-invent.loca.lt\n"
          ]
        }
      ]
    }
  ]
}