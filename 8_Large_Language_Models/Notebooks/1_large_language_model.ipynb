{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models\n",
        "\n",
        "In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM architectures, LangChain stands out for its efficiency and flexibility.\n",
        "\n",
        "This notebook is designed to seamlessly run both locally and on Google Colab. For those who may only have a CPU, there are clear instructions on how to run the notebook without a GPU. Don't worry, simply follow the instructions for either GPU or CPU, depending on your setup.\n",
        "\n",
        "Please note that using only a CPU will result in noticeably slower model performance."
      ],
      "metadata": {
        "id": "rQEWtbLY3-xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
        "\n",
        "On Google Colab, you have access to free GPUs, whenever they're available. Let's utilise this advantage. To configure a Colab GPU, navigate to \"Edit\" and then \"Notebook Settings\". Select \"GPU\" and then click \"Save\".\n",
        "\n",
        "To proceed, you'll need to install two libraries: Langchain and Llama.cpp. When operating this notebook locally, you only need to install these libraries once, and they'll remain on your computer. However, in Colab, they're not default libraries and must be installed for each session.\n",
        "\n",
        "**LangChain** is a framework that simplifies the development of applications powered by large language models (LLMs)\n",
        "\n",
        "**llama.cpp** enables us to execute quantised versions of models.\n",
        "\n",
        "> Quantisation of LLMs is a process that reduces the precision of the numerical values in the model, such as converting 32-bit floating-point numbers to 8-bit integers. These models are therefore smaller and faster, allowing them to run on less powerful hardware with only a small loss in precision.\n",
        "\n",
        "* If you're using a **CPU**, use the [standard installation](https://python.langchain.com/docs/integrations/llms/llamacpp#cpu-only-installation) of llama.cpp. Windows users might have to install [a couple of extra libraries too](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-windows). Some students using windows have also found [this guide](https://medium.com/@piyushbatra1999/installing-llama-cpp-python-with-nvidia-gpu-acceleration-on-windows-a-short-guide-0dfac475002d) useful.\n",
        "* If you have an **NVIDIA GPU**, you need to [activate cuBLAS](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-openblas-cublas-clblast) with llama.cpp. cuBLAS is a library that speeds up operations on NVIDIA GPUs.\n",
        "* If you have a **silicon chip Apple with a GPU**, you need to [enable Metal](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-metal)."
      ],
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "f7bb55df-79de-4a3b-802d-53a0d4ca4b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "# As this notebook is originally on Colab, here we'll use their NVIDIA GPU and activate cuBLAS\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into the examples, let's download the large language model (LLM) we'll be using. For these exercises, we've selected a [quantised version of Mistral AI's Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). While this is a great choice, it's by no means the only option. We encourage you to explore and try different models to discover the unique strengths and weaknesses of each. Even models of similar size can exhibit surprisingly different capabilities.\n",
        "\n",
        "> Since we're working in Colab, we'll need to download the LLM for each session.\n",
        "<br>\n",
        "If you're working locally, you can download the model once. The model is then on your computer and doesn't need to be downloaded each time. Change the `--local-dir` to your folder of choice."
      ],
      "metadata": {
        "id": "XIolHmA0qIpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKugu_x6qDbj",
        "outputId": "f5a56a52-1db9-4698-a9e9-bb4e2741c3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp8a1s5d58\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:39<00:00, 111MB/s] \n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM üß†\n",
        "\n",
        "Langchain simplifies LLM deployment with its streamlined setup process. A single line of code configures your LLM, allowing you to tailor the parameters to your specific needs.\n",
        "\n",
        "If you want to know more about Llama.cpp, you can [read the docs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/). Alternatively, here are the [LangChain docs for Llama.cpp](https://python.langchain.com/docs/integrations/llms/llamacpp).\n",
        "\n",
        "Here's a brief overview of some of the parameters:\n",
        "* **model_path:** The path to the Llama model file that will be used for generating text.\n",
        "* **max_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **n_gpu_layers:** The default setting of 0 will cause all layers to be executed on the CPU. Setting n_gpu_layers to 1 will cause the first layer of the model to be executed on the GPU, while the remaining layers are executed on the CPU. Setting n_gpu_layers to 2 will cause the first two layers of the model to be executed on the GPU, while the remaining layers are executed on the CPU, and so on. -1 will cause all layers to be offloaded to the GPU. In general, it is a good idea to experiment with different values of n_gpu_layers to find the best balance between performance and memory usage for your specific application."
      ],
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F76VzycqhLv",
        "outputId": "80ba035a-668d-4f07-b086-2191dad9b44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.14 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.14 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're using a GPU, check the output of this cell ‚òùÔ∏è\n",
        "  * If you're using cuBLAS, you'll see `BLAS = 1` if it's installed correctly.\n",
        "  * If you're using Metal, you'll see `NEON = 1` if it's installed correctly."
      ],
      "metadata": {
        "id": "OxhMbwWZjyOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3.&nbsp; Asking your LLM questions ü§ñ\n",
        "Play around and note how small changes make a big difference."
      ],
      "metadata": {
        "id": "QWG2XPn15NgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n",
        "print(answer_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCFyGekD1YWQ",
        "outputId": "168512bc-aecb-4239-de70-fbe657eec97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     149.94 ms /   226 runs   (    0.66 ms per token,  1507.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     232.30 ms /     8 tokens (   29.04 ms per token,    34.44 tokens per second)\n",
            "llama_print_timings:        eval time =    5334.07 ms /   226 runs   (   23.60 ms per token,    42.37 tokens per second)\n",
            "llama_print_timings:       total time =    6673.09 ms /   234 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Polar Bears\n",
            "2. Arctic Foxes\n",
            "3. Walruses\n",
            "4. Caribou\n",
            "5. Beluga Whales\n",
            "6. Narwhals\n",
            "7. Seals\n",
            "8. Musk Oxen\n",
            "9. Arctic Hares\n",
            "10. Snowy Owls\n",
            "11. Reindeer\n",
            "12. Beavers\n",
            "13. Moose\n",
            "14. Lynx\n",
            "15. Wolverines\n",
            "16. Arctic Wolves\n",
            "17. Harp Seals\n",
            "18. Dall Sheep\n",
            "19. Pacific Walruses\n",
            "20. Harp Porpoises\n",
            "21. Pacific Salmon\n",
            "22. Arctic Char\n",
            "23. Dwarf Arctic Foxes\n",
            "24. Arctic Poppies\n",
            "25. Arctic Willows\n",
            "26. Arctic Cotton\n",
            "27. Arctic Cranberries\n",
            "28. Arctic Blueberries\n",
            "29. Arctic Crowberries\n",
            "30. Arctic Huckleberries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n",
        "print(answer_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8mzzu51d5b",
        "outputId": "cb182764-7990-49d3-a5b5-733b9d91ce5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     166.65 ms /   299 runs   (    0.56 ms per token,  1794.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     110.36 ms /    12 tokens (    9.20 ms per token,   108.73 tokens per second)\n",
            "llama_print_timings:        eval time =    6941.35 ms /   298 runs   (   23.29 ms per token,    42.93 tokens per second)\n",
            "llama_print_timings:       total time =    8131.73 ms /   310 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "In the land of snow and ice,\n",
            "Where the sun doesn't shine nice,\n",
            "Lives a group of animals brave,\n",
            "Adapted to their cold environment.\n",
            "\n",
            "The polar bear is the king,\n",
            "Of this arctic realm so thick,\n",
            "With fur so white and thick,\n",
            "It keeps him warm from the cold trick.\n",
            "\n",
            "He hunts for seals to eat,\n",
            "On the frozen sea he meets,\n",
            "With his powerful paws and teeth,\n",
            "He can break through the thickest ice sheet.\n",
            "\n",
            "The arctic fox is next,\n",
            "With his coat of white and red,\n",
            "He blends in with his surroundings,\n",
            "And can run fast when he needs.\n",
            "\n",
            "He eats lemmings and birds,\n",
            "And keeps warm with his fur,\n",
            "His small size helps him conserve,\n",
            "Energy in this harsh world.\n",
            "\n",
            "The caribou roam free,\n",
            "With antlers tall and wide,\n",
            "They eat grasses and lichens,\n",
            "And can run fast when they need to hide.\n",
            "\n",
            "The arctic hare is small,\n",
            "But quick as a cheetah,\n",
            "He eats the leftovers,\n",
            "When the caribou have had enough.\n",
            "\n",
            "These animals live in harmony,\n",
            "In this land of snow and ice,\n",
            "They have adapted to their surroundings,\n",
            "And can survive in this cold paradise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n",
        "print(answer_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3LAlLON1G3g",
        "outputId": "ed3174f7-2282-453f-ae96-40f4a3b0f7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =      86.72 ms /   136 runs   (    0.64 ms per token,  1568.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     134.51 ms /    15 tokens (    8.97 ms per token,   111.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3147.49 ms /   135 runs   (   23.31 ms per token,    42.89 tokens per second)\n",
            "llama_print_timings:       total time =    3836.48 ms /   150 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The central limit theorem is a math rule that says when you take lots of numbers and add them up, it doesn't really matter where each number came from or how many there are - the result will still be pretty much normal. Like if you have a bunch of apples and bananas and you mix them together, the height of the pile won't be exactly average (because apples and bananas have different heights), but it will be pretty close to average. And if you have even more apples and bananas, the pile will get even closer to average. The more numbers you add, the closer the result gets to the middle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."
      ],
      "metadata": {
        "id": "AnvnmmRIYQhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge üòÄ\n",
        "Play around with this, and other, LLMs. keep a record of your findings:\n",
        "1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n",
        "2. Experiment with the parameters, one at a time, to assess their impact on the output.\n",
        "3. Attempt to load different models: Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. Then use the filter bar for `GGUF` to find already quantised models.\n",
        "\n",
        "You can alter the download command accordingly. In this note book we used the command:"
      ],
      "metadata": {
        "id": "WD4i8-3FEp04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "Rzkwz4cI1DgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the version `mistral-7b-instruct-v0.1.Q4_K_M.gguf` of the model `TheBloke/Mistral-7B-Instruct-v0.1-GGUF` from huggingface. You can read about the different versions on the models `model card`.\n",
        "\n",
        "To adapt this just change the model and the version to your new choice.\n",
        "\n",
        "`!huggingface-cli download {model_name} {model_version} --local-dir . --local-dir-use-symlinks False`\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "fCADTg0j1xnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "R9ChvulP1Dd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above would download a [quantised version of Meta's Llama 2 7B chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IouC-hN42PHc"
      }
    }
  ]
}