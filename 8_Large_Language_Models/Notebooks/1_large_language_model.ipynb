{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM architectures, LangChain stands out for its efficiency and flexibility.\n",
        "\n",
        "This notebook is designed to seamlessly run both locally and on Google Colab. For those who may only have a CPU, there are clear instructions on how to run the notebook without a GPU. Don't worry, simply follow the instructions for either GPU or CPU, depending on your setup.\n",
        "\n",
        "Please note that using only a CPU will result in noticeably slower model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings 🛠️\n",
        "\n",
        "On Google Colab, you have access to free GPUs, whenever they're available. Let's utilise this advantage. To configure a Colab GPU, navigate to \"Edit\" and then \"Notebook Settings\". Select \"GPU\" and then click \"Save\".\n",
        "\n",
        "To proceed, you'll need to install two libraries: Langchain and Llama.cpp. When operating this notebook locally, you only need to install these libraries once, and they'll remain on your computer. However, in Colab, they're not default libraries and must be installed for each session.\n",
        "\n",
        "**LangChain** is a framework that simplifies the development of applications powered by large language models (LLMs)\n",
        "\n",
        "**llama.cpp** enables us to execute quantised versions of models.\n",
        "\n",
        "> Quantisation of LLMs is a process that reduces the precision of the numerical values in the model, such as converting 32-bit floating-point numbers to 8-bit integers. These models are therefore smaller and faster, allowing them to run on less powerful hardware with only a small loss in precision.\n",
        "\n",
        "* If you're using a **CPU**, use the [standard installation](https://python.langchain.com/docs/integrations/llms/llamacpp#cpu-only-installation) of llama.cpp. Windows users might have to install [a couple of extra libraries too](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-windows). Some students using windows have also found [this guide](https://medium.com/@piyushbatra1999/installing-llama-cpp-python-with-nvidia-gpu-acceleration-on-windows-a-short-guide-0dfac475002d) useful.\n",
        "* If you have an **NVIDIA GPU**, you need to [activate cuBLAS](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-openblas-cublas-clblast) with llama.cpp. cuBLAS is a library that speeds up operations on NVIDIA GPUs.\n",
        "* If you have a **silicon chip Apple with a GPU**, you need to [enable Metal](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-metal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (3.29.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install cmake\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUDA_PATH = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "!pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "f7bb55df-79de-4a3b-802d-53a0d4ca4b1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Der Befehl \"CMAKE_ARGS\" ist entweder falsch geschrieben oder\n",
            "konnte nicht gefunden werden.\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "# As this notebook is originally on Colab, here we'll use their NVIDIA GPU and activate cuBLAS\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --force-reinstall --upgrade --no-cache-dir --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Der Befehl \"CMAKE_ARGS\" ist entweder falsch geschrieben oder\n",
            "konnte nicht gefunden werden.\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.61.tar.gz (37.4 MB)\n",
            "     ---------------------------------------- 0.0/37.4 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/37.4 MB 3.2 MB/s eta 0:00:12\n",
            "     ---------------------------------------- 0.4/37.4 MB 4.4 MB/s eta 0:00:09\n",
            "      --------------------------------------- 0.6/37.4 MB 5.0 MB/s eta 0:00:08\n",
            "     - -------------------------------------- 1.0/37.4 MB 5.3 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 1.3/37.4 MB 5.7 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 1.4/37.4 MB 5.4 MB/s eta 0:00:07\n",
            "     - -------------------------------------- 1.5/37.4 MB 4.7 MB/s eta 0:00:08\n",
            "     -- ------------------------------------- 2.4/37.4 MB 6.6 MB/s eta 0:00:06\n",
            "     -- ------------------------------------- 2.7/37.4 MB 6.7 MB/s eta 0:00:06\n",
            "     --- ------------------------------------ 2.9/37.4 MB 6.6 MB/s eta 0:00:06\n",
            "     --- ------------------------------------ 3.4/37.4 MB 6.7 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 3.8/37.4 MB 6.7 MB/s eta 0:00:06\n",
            "     ---- ----------------------------------- 4.1/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     ---- ----------------------------------- 4.5/37.4 MB 6.8 MB/s eta 0:00:05\n",
            "     ----- ---------------------------------- 4.8/37.4 MB 6.8 MB/s eta 0:00:05\n",
            "     ----- ---------------------------------- 5.1/37.4 MB 7.0 MB/s eta 0:00:05\n",
            "     ----- ---------------------------------- 5.3/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     ------ --------------------------------- 5.8/37.4 MB 7.0 MB/s eta 0:00:05\n",
            "     ------ --------------------------------- 6.2/37.4 MB 7.0 MB/s eta 0:00:05\n",
            "     ------ --------------------------------- 6.5/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     ------- -------------------------------- 6.9/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     ------- -------------------------------- 7.2/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 7.5/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 7.9/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 8.1/37.4 MB 7.1 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 8.3/37.4 MB 6.9 MB/s eta 0:00:05\n",
            "     -------- ------------------------------- 8.3/37.4 MB 6.9 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 8.7/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 9.0/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 9.3/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 9.7/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     ---------- ----------------------------- 10.0/37.4 MB 6.7 MB/s eta 0:00:05\n",
            "     ----------- ---------------------------- 10.4/37.4 MB 6.9 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 10.7/37.4 MB 7.0 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 11.1/37.4 MB 7.0 MB/s eta 0:00:04\n",
            "     ------------ --------------------------- 11.4/37.4 MB 7.0 MB/s eta 0:00:04\n",
            "     ------------ --------------------------- 11.7/37.4 MB 7.4 MB/s eta 0:00:04\n",
            "     ------------ --------------------------- 12.0/37.4 MB 7.1 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 12.2/37.4 MB 7.0 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 12.6/37.4 MB 6.8 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 12.7/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ------------- -------------------------- 13.1/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 13.4/37.4 MB 6.8 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 13.8/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 14.1/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 14.5/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     --------------- ------------------------ 14.8/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 15.1/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 15.5/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 15.8/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 16.2/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ----------------- ---------------------- 16.5/37.4 MB 6.8 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 16.9/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 17.2/37.4 MB 6.7 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 17.6/37.4 MB 6.7 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 17.9/37.4 MB 6.7 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 18.0/37.4 MB 6.6 MB/s eta 0:00:03\n",
            "     ------------------- -------------------- 18.4/37.4 MB 6.8 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 18.7/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 19.1/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 19.4/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 19.8/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 20.1/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 20.5/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 20.8/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 21.2/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 21.5/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 21.8/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 22.2/37.4 MB 7.0 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 22.6/37.4 MB 7.1 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 22.9/37.4 MB 7.3 MB/s eta 0:00:03\n",
            "     ------------------------ --------------- 23.2/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 23.6/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 23.9/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 24.3/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     -------------------------- ------------- 24.6/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     -------------------------- ------------- 25.0/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 25.3/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 25.7/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 26.0/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ---------------------------- ----------- 26.4/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ---------------------------- ----------- 26.7/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ---------------------------- ----------- 27.0/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 27.4/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 27.7/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 28.1/37.4 MB 7.3 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 28.4/37.4 MB 7.4 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 28.7/37.4 MB 7.4 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 29.1/37.4 MB 7.4 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 29.4/37.4 MB 7.4 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 29.8/37.4 MB 7.4 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 30.2/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 30.5/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 30.8/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 31.2/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 31.5/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 31.9/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 32.2/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 32.6/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 32.9/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 33.3/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 33.6/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 34.0/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 34.3/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 34.6/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 35.0/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 35.3/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 35.7/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 36.0/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 36.4/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  36.7/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.0/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  37.3/37.4 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 37.4/37.4 MB 7.4 MB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
            "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 61.0/61.0 kB ? eta 0:00:00\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
            "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
            "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "   ---------------------------------------- 0.0/133.2 kB ? eta -:--:--\n",
            "   ---------------------------------------- 133.2/133.2 kB 7.7 MB/s eta 0:00:00\n",
            "Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
            "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.4/15.8 MB 7.6 MB/s eta 0:00:03\n",
            "   - -------------------------------------- 0.7/15.8 MB 7.6 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 1.1/15.8 MB 7.5 MB/s eta 0:00:02\n",
            "   --- ------------------------------------ 1.3/15.8 MB 6.7 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 1.6/15.8 MB 6.8 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.0/15.8 MB 6.6 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.0/15.8 MB 6.1 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.3/15.8 MB 5.9 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 2.7/15.8 MB 6.3 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 2.9/15.8 MB 6.0 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 3.2/15.8 MB 6.2 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 3.6/15.8 MB 6.2 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 3.8/15.8 MB 6.1 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 4.2/15.8 MB 6.3 MB/s eta 0:00:02\n",
            "   ----------- ---------------------------- 4.6/15.8 MB 6.5 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 4.9/15.8 MB 6.4 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 5.3/15.8 MB 6.5 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 5.6/15.8 MB 6.5 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 6.0/15.8 MB 6.6 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 6.3/15.8 MB 6.6 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 6.7/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 6.8/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 7.2/15.8 MB 6.6 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 7.5/15.8 MB 6.6 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 7.9/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 8.3/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 8.6/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 9.0/15.8 MB 6.7 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 9.3/15.8 MB 6.8 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 9.7/15.8 MB 6.8 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 10.0/15.8 MB 6.8 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 10.4/15.8 MB 6.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 10.7/15.8 MB 6.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 11.0/15.8 MB 6.9 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 11.4/15.8 MB 6.9 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 11.8/15.8 MB 7.0 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 12.1/15.8 MB 7.0 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 12.4/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 12.8/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 13.1/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 13.5/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 13.8/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 14.2/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 14.5/15.8 MB 7.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 14.9/15.8 MB 7.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 15.2/15.8 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  15.6/15.8 MB 7.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 15.8/15.8 MB 7.2 MB/s eta 0:00:00\n",
            "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
            "Failed to build llama-cpp-python\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [46 lines of output]\n",
            "      \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.2\u001b[0m using \u001b[94mCMake 3.29.2\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
            "      \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
            "      2024-04-15 18:20:54,213 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
            "      loading initial cache file C:\\Users\\Marvin\\AppData\\Local\\Temp\\tmptgi7769c\\build\\CMakeInit.txt\n",
            "      -- Building for: Visual Studio 17 2022\n",
            "      -- Selecting Windows SDK version 10.0.20348.0 to target Windows 10.0.19045.\n",
            "      -- The C compiler identification is MSVC 19.39.33523.0\n",
            "      -- The CXX compiler identification is MSVC 19.39.33523.0\n",
            "      -- Detecting C compiler ABI info\n",
            "      -- Detecting C compiler ABI info - done\n",
            "      -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2022/BuildTools/VC/Tools/MSVC/14.39.33519/bin/Hostx64/x64/cl.exe - skipped\n",
            "      -- Detecting C compile features\n",
            "      -- Detecting C compile features - done\n",
            "      -- Detecting CXX compiler ABI info\n",
            "      -- Detecting CXX compiler ABI info - done\n",
            "      -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2022/BuildTools/VC/Tools/MSVC/14.39.33519/bin/Hostx64/x64/cl.exe - skipped\n",
            "      -- Detecting CXX compile features\n",
            "      -- Detecting CXX compile features - done\n",
            "      -- Found Git: C:/Program Files/Git/cmd/git.exe (found version \"2.43.0.windows.1\")\n",
            "      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "      -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "      -- Looking for pthread_create in pthreads\n",
            "      -- Looking for pthread_create in pthreads - not found\n",
            "      -- Looking for pthread_create in pthread\n",
            "      -- Looking for pthread_create in pthread - not found\n",
            "      -- Found Threads: TRUE\n",
            "      CMake Warning at vendor/llama.cpp/CMakeLists.txt:376 (message):\n",
            "        LLAMA_CUBLAS is deprecated and will be removed in the future.\n",
            "      \n",
            "        Use LLAMA_CUDA instead\n",
            "      \n",
            "      \n",
            "      -- Found CUDAToolkit: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2/include (found version \"12.2.91\")\n",
            "      -- CUDA found\n",
            "      CMake Error at C:/Users/Marvin/AppData/Local/Temp/pip-build-env-e01pdjal/normal/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:563 (message):\n",
            "        No CUDA toolset found.\n",
            "      Call Stack (most recent call first):\n",
            "        C:/Users/Marvin/AppData/Local/Temp/pip-build-env-e01pdjal/normal/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:8 (CMAKE_DETERMINE_COMPILER_ID_BUILD)\n",
            "        C:/Users/Marvin/AppData/Local/Temp/pip-build-env-e01pdjal/normal/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCompilerId.cmake:53 (__determine_compiler_id_test)\n",
            "        C:/Users/Marvin/AppData/Local/Temp/pip-build-env-e01pdjal/normal/Lib/site-packages/cmake/data/share/cmake-3.29/Modules/CMakeDetermineCUDACompiler.cmake:131 (CMAKE_DETERMINE_COMPILER_ID)\n",
            "        vendor/llama.cpp/CMakeLists.txt:387 (enable_language)\n",
            "      \n",
            "      \n",
            "      -- Configuring incomplete, errors occurred!\n",
            "      \n",
            "      \u001b[91m\u001b[1m*** CMake configuration failed\u001b[0m\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for llama-cpp-python\n",
            "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CMAKE_ARGS'] = \"-DLLAMA_CUBLAS=on\"\n",
        "os.environ['FORCE_CMAKE'] = \"1\"\n",
        "!pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIolHmA0qIpT"
      },
      "source": [
        "Before we dive into the examples, let's download the large language model (LLM) we'll be using. For these exercises, we've selected a [quantised version of Mistral AI's Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). While this is a great choice, it's by no means the only option. We encourage you to explore and try different models to discover the unique strengths and weaknesses of each. Even models of similar size can exhibit surprisingly different capabilities.\n",
        "\n",
        "> Since we're working in Colab, we'll need to download the LLM for each session.\n",
        "<br>\n",
        "If you're working locally, you can download the model once. The model is then on your computer and doesn't need to be downloaded each time. Change the `--local-dir` to your folder of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting filelock (from huggingface_hub)\n",
            "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
            "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 57.6/57.6 kB 1.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marvin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
            "   --- ------------------------------------ 30.7/388.9 kB ? eta -:--:--\n",
            "   ------------ --------------------------- 122.9/388.9 kB 2.4 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 204.8/388.9 kB 2.1 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 256.0/388.9 kB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 388.9/388.9 kB 2.0 MB/s eta 0:00:00\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
            "   ---------------------------- ----------- 122.9/172.0 kB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 172.0/172.0 kB 2.6 MB/s eta 0:00:00\n",
            "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
            "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
            "Downloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: tqdm, fsspec, filelock, huggingface_hub\n",
            "Successfully installed filelock-3.13.4 fsspec-2024.3.1 huggingface_hub-0.22.2 tqdm-4.66.2\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Marvin\\Documents\\WBS\\Data-Science-Bootcamp\\8_Large_Language_Models\\LLMs\\mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to C:\\Users\\Marvin\\.cache\\huggingface\\hub\\tmpwebqpifi\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir C:\\Users\\Marvin\\Documents\\WBS\\Data-Science-Bootcamp\\8_Large_Language_Models\\LLMs --local-dir-use-symlinks False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM 🧠\n",
        "\n",
        "Langchain simplifies LLM deployment with its streamlined setup process. A single line of code configures your LLM, allowing you to tailor the parameters to your specific needs.\n",
        "\n",
        "If you want to know more about Llama.cpp, you can [read the docs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/). Alternatively, here are the [LangChain docs for Llama.cpp](https://python.langchain.com/docs/integrations/llms/llamacpp).\n",
        "\n",
        "Here's a brief overview of some of the parameters:\n",
        "* **model_path:** The path to the Llama model file that will be used for generating text.\n",
        "* **max_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **n_gpu_layers:** The default setting of 0 will cause all layers to be executed on the CPU. Setting n_gpu_layers to 1 will cause the first layer of the model to be executed on the GPU, while the remaining layers are executed on the CPU. Setting n_gpu_layers to 2 will cause the first two layers of the model to be executed on the GPU, while the remaining layers are executed on the CPU, and so on. -1 will cause all layers to be offloaded to the GPU. In general, it is a good idea to experiment with different values of n_gpu_layers to find the best balance between performance and memory usage for your specific application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F76VzycqhLv",
        "outputId": "80ba035a-668d-4f07-b086-2191dad9b44c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:\\Users\\Marvin\\Documents\\WBS\\Data-Science-Bootcamp\\8_Large_Language_Models\\LLMs\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     1.27 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = r\"C:\\Users\\Marvin\\Documents\\WBS\\Data-Science-Bootcamp\\8_Large_Language_Models\\LLMs\\mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxhMbwWZjyOm"
      },
      "source": [
        "If you're using a GPU, check the output of this cell ☝️\n",
        "  * If you're using cuBLAS, you'll see `BLAS = 1` if it's installed correctly.\n",
        "  * If you're using Metal, you'll see `NEON = 1` if it's installed correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Asking your LLM questions 🤖\n",
        "Play around and note how small changes make a big difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCFyGekD1YWQ",
        "outputId": "168512bc-aecb-4239-de70-fbe657eec97e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     149.94 ms /   226 runs   (    0.66 ms per token,  1507.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     232.30 ms /     8 tokens (   29.04 ms per token,    34.44 tokens per second)\n",
            "llama_print_timings:        eval time =    5334.07 ms /   226 runs   (   23.60 ms per token,    42.37 tokens per second)\n",
            "llama_print_timings:       total time =    6673.09 ms /   234 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Polar Bears\n",
            "2. Arctic Foxes\n",
            "3. Walruses\n",
            "4. Caribou\n",
            "5. Beluga Whales\n",
            "6. Narwhals\n",
            "7. Seals\n",
            "8. Musk Oxen\n",
            "9. Arctic Hares\n",
            "10. Snowy Owls\n",
            "11. Reindeer\n",
            "12. Beavers\n",
            "13. Moose\n",
            "14. Lynx\n",
            "15. Wolverines\n",
            "16. Arctic Wolves\n",
            "17. Harp Seals\n",
            "18. Dall Sheep\n",
            "19. Pacific Walruses\n",
            "20. Harp Porpoises\n",
            "21. Pacific Salmon\n",
            "22. Arctic Char\n",
            "23. Dwarf Arctic Foxes\n",
            "24. Arctic Poppies\n",
            "25. Arctic Willows\n",
            "26. Arctic Cotton\n",
            "27. Arctic Cranberries\n",
            "28. Arctic Blueberries\n",
            "29. Arctic Crowberries\n",
            "30. Arctic Huckleberries\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8mzzu51d5b",
        "outputId": "cb182764-7990-49d3-a5b5-733b9d91ce5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     166.65 ms /   299 runs   (    0.56 ms per token,  1794.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     110.36 ms /    12 tokens (    9.20 ms per token,   108.73 tokens per second)\n",
            "llama_print_timings:        eval time =    6941.35 ms /   298 runs   (   23.29 ms per token,    42.93 tokens per second)\n",
            "llama_print_timings:       total time =    8131.73 ms /   310 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "In the land of snow and ice,\n",
            "Where the sun doesn't shine nice,\n",
            "Lives a group of animals brave,\n",
            "Adapted to their cold environment.\n",
            "\n",
            "The polar bear is the king,\n",
            "Of this arctic realm so thick,\n",
            "With fur so white and thick,\n",
            "It keeps him warm from the cold trick.\n",
            "\n",
            "He hunts for seals to eat,\n",
            "On the frozen sea he meets,\n",
            "With his powerful paws and teeth,\n",
            "He can break through the thickest ice sheet.\n",
            "\n",
            "The arctic fox is next,\n",
            "With his coat of white and red,\n",
            "He blends in with his surroundings,\n",
            "And can run fast when he needs.\n",
            "\n",
            "He eats lemmings and birds,\n",
            "And keeps warm with his fur,\n",
            "His small size helps him conserve,\n",
            "Energy in this harsh world.\n",
            "\n",
            "The caribou roam free,\n",
            "With antlers tall and wide,\n",
            "They eat grasses and lichens,\n",
            "And can run fast when they need to hide.\n",
            "\n",
            "The arctic hare is small,\n",
            "But quick as a cheetah,\n",
            "He eats the leftovers,\n",
            "When the caribou have had enough.\n",
            "\n",
            "These animals live in harmony,\n",
            "In this land of snow and ice,\n",
            "They have adapted to their surroundings,\n",
            "And can survive in this cold paradise.\n"
          ]
        }
      ],
      "source": [
        "answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n",
        "print(answer_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3LAlLON1G3g",
        "outputId": "ed3174f7-2282-453f-ae96-40f4a3b0f7c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =      86.72 ms /   136 runs   (    0.64 ms per token,  1568.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     134.51 ms /    15 tokens (    8.97 ms per token,   111.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3147.49 ms /   135 runs   (   23.31 ms per token,    42.89 tokens per second)\n",
            "llama_print_timings:       total time =    3836.48 ms /   150 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The central limit theorem is a math rule that says when you take lots of numbers and add them up, it doesn't really matter where each number came from or how many there are - the result will still be pretty much normal. Like if you have a bunch of apples and bananas and you mix them together, the height of the pile won't be exactly average (because apples and bananas have different heights), but it will be pretty close to average. And if you have even more apples and bananas, the pile will get even closer to average. The more numbers you add, the closer the result gets to the middle.\n"
          ]
        }
      ],
      "source": [
        "answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnvnmmRIYQhF"
      },
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD4i8-3FEp04"
      },
      "source": [
        "---\n",
        "## 4.&nbsp; Challenge 😀\n",
        "Play around with this, and other, LLMs. keep a record of your findings:\n",
        "1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n",
        "2. Experiment with the parameters, one at a time, to assess their impact on the output.\n",
        "3. Attempt to load different models: Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. Then use the filter bar for `GGUF` to find already quantised models.\n",
        "\n",
        "You can alter the download command accordingly. In this note book we used the command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzkwz4cI1DgP"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCADTg0j1xnU"
      },
      "source": [
        "This downloads the version `mistral-7b-instruct-v0.1.Q4_K_M.gguf` of the model `TheBloke/Mistral-7B-Instruct-v0.1-GGUF` from huggingface. You can read about the different versions on the models `model card`.\n",
        "\n",
        "To adapt this just change the model and the version to your new choice.\n",
        "\n",
        "`!huggingface-cli download {model_name} {model_version} --local-dir . --local-dir-use-symlinks False`\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ChvulP1Dd0"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IouC-hN42PHc"
      },
      "source": [
        "The code above would download a [quantised version of Meta's Llama 2 7B chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
