{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjATpOsWQFem",
        "outputId": "e5f40966-d59d-44ec-81f9-9b190b5e49fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf6nwlPx9sFV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/drive/MyDrive/8_Generative_AI_CPU/lib/python3.10/site-packages')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXrt2EG1-dj9",
        "outputId": "ff8ca580-7ada-4180-c069-66033536eaa1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/drive/MyDrive/LLMs/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     2.56 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path=\"/content/drive/MyDrive/LLMs/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugoqbXiKfZSi",
        "outputId": "9ba972eb-0092-4240-eb3a-08163cb47d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp2_yvegk_\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:35<00:00, 122MB/s]\n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OJKuawHV1pr",
        "outputId": "255fc7bb-4cdb-422a-ad96-47037bea9989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.25.3-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.13.4)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.2.0)\n",
            "Installing collected packages: distlib, virtualenv\n",
            "Successfully installed distlib-0.3.8 virtualenv-20.25.3\n"
          ]
        }
      ],
      "source": [
        "!pip install virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRobtDbCsia4",
        "outputId": "817717ff-b3f3-4513-b3cf-e3917c0c4e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "created virtual environment CPython3.10.12.final.0-64 in 1011ms\n",
            "  creator CPython3Posix(dest=/content, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.0, setuptools==69.5.1, wheel==0.43.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "!virtualenv /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8scZDcWCEA",
        "outputId": "efd502cd-afcf-41c9-cdd2-920f295c26f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "created virtual environment CPython3.10.12.final.0-64 in 26317ms\n",
            "  creator CPython3Posix(dest=/content/drive/MyDrive/8_Generative_AI_CPU, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: Jinja2==3.1.3, MarkupSafe==2.1.5, PyYAML==6.0.1, SQLAlchemy==2.0.29, aiohttp==3.9.5, aiosignal==1.3.1, annotated_types==0.6.0, async_timeout==4.0.3, attrs==23.2.0, certifi==2024.2.2, charset_normalizer==3.3.2, dataclasses_json==0.6.4, diskcache==5.6.3, faiss_cpu==1.8.0, filelock==3.13.4, frozenlist==1.4.1, fsspec==2024.3.1, greenlet==3.0.3, huggingface_hub==0.22.2, idna==3.7, joblib==1.4.0, jsonpatch==1.33, jsonpointer==2.4, langchain==0.1.16, langchain_community==0.0.33, langchain_core==0.1.43, langchain_text_splitters==0.0.1, langsmith==0.1.48, llama_cpp_python==0.2.61, marshmallow==3.21.1, mpmath==1.3.0, multidict==6.0.5, mypy_extensions==1.0.0, networkx==3.3, numpy==1.26.4, nvidia_cublas_cu12==12.1.3.1, nvidia_cuda_cupti_cu12==12.1.105, nvidia_cuda_nvrtc_cu12==12.1.105, nvidia_cuda_runtime_cu12==12.1.105, nvidia_cudnn_cu12==8.9.2.26, nvidia_cufft_cu12==11.0.2.54, nvidia_curand_cu12==10.3.2.106, nvidia_cusolver_cu12==11.4.5.107, nvidia_cusparse_cu12==12.1.0.106, nvidia_nccl_cu12==2.19.3, nvidia_nvjitlink_cu12==12.4.127, nvidia_nvtx_cu12==12.1.105, orjson==3.10.1, packaging==23.2, pillow==10.3.0, pip==24.0, pydantic==2.7.0, pydantic_core==2.18.1, pypdf==4.2.0, regex==2024.4.16, requests==2.31.0, safetensors==0.4.3, scikit_learn==1.4.2, scipy==1.13.0, sentence_transformers==2.7.0, setuptools==69.5.1, sympy==1.12, tenacity==8.2.3, threadpoolctl==3.4.0, tokenizers==0.15.2, torch==2.2.2, tqdm==4.66.2, transformers==4.39.3, triton==2.2.0, typing_extensions==4.11.0, typing_inspect==0.9.0, urllib3==2.2.1, wheel==0.43.0, yarl==1.9.4\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ],
      "source": [
        "!virtualenv /content/drive/MyDrive/8_Generative_AI_CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8CUECzqT0kf"
      },
      "outputs": [],
      "source": [
        "!source /content/drive/MyDrive/8_Generative_AI/bin/activate && pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24B7sbRHhhlu",
        "outputId": "7cd87bfd-f46e-40f2-feb1-3d41b23af18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!source /content/drive/MyDrive/8_Generative_AI_CPU/bin/activate && pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Lu2KSAvST6hJ",
        "outputId": "5a76a77c-0698-438a-8c01-116cfd680352"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76a43feb-e026-4d2a-adaf-88ceb2a3b34e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76a43feb-e026-4d2a-adaf-88ceb2a3b34e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving harvard computer learning humor.pdf to harvard computer learning humor.pdf\n",
            "Saving jokes1.pdf to jokes1.pdf\n",
            "Saving jokes2.pdf to jokes2.pdf\n",
            "Saving jokes3.pdf to jokes3.pdf\n",
            "Saving Jokes4.pdf to Jokes4.pdf\n",
            "Saving Jokes5.pdf to Jokes5.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gesamtanzahl der Seiten: 215\n",
            "Inhalt der ersten Seite:\n",
            "page_content='The funniest  one and only \\nmultilingual joke book\\nEUROPEAN CENTRE FOR\\nMODERN LANGU AGES\\nCENTRE EUROPEEN POUR\\nLES LANGUES VIVANTESEuropean Day of Languages \\nSeptember 26\\n   HA \\n  HA \\nHAHØ \\n  HØ \\n    HØ\\nJA\\nJAJAHÉHÉ\\nHÉK\\nKKKK' metadata={'source': 'Jokes5.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Lade die PDF-Dateien hoch\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Erstelle eine Liste der Dateinamen\n",
        "file_names = list(uploaded_files.keys())\n",
        "\n",
        "# Initialisiere eine leere Liste für die Seiten der PDF-Dateien\n",
        "all_pages = []\n",
        "\n",
        "# Iteriere über die hochgeladenen Dateien\n",
        "for file_name in file_names:\n",
        "    # Erstelle einen PyPDFLoader und lade das PDF-Dokument\n",
        "    loader = PyPDFLoader(file_name)\n",
        "    pages = loader.load_and_split()\n",
        "    all_pages.extend(pages)\n",
        "\n",
        "# Gib die Anzahl der Seiten aus\n",
        "print(\"Gesamtanzahl der Seiten:\", len(all_pages))\n",
        "\n",
        "# Gib den Inhalt der ersten Seite aus\n",
        "print(\"Inhalt der ersten Seite:\")\n",
        "print(pages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuBEH1Ftwi3o",
        "outputId": "3bc11c03-2a56-4ad9-d382-3be6ce01a318"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkBrmTfhdURa"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnEfBciRj2sc"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
        "                                               chunk_overlap=150)\n",
        "\n",
        "docs = text_splitter.split_documents(all_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U_ARRehdNbJ"
      },
      "outputs": [],
      "source": [
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbrashb0kKej"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJ_CsKZktUB"
      },
      "outputs": [],
      "source": [
        "vector_db.save_local(\"/content/faiss_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i6SDpPEXDNN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# load vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history',\n",
        "                                  return_messages=True,\n",
        "                                  output_key='answer')\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "  (4) Be funny.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-DnxOpElz7T",
        "outputId": "0c34727f-41c2-4815-fd40-8d12d7cef183"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    5947.07 ms\n",
            "llama_print_timings:      sample time =       9.19 ms /    14 runs   (    0.66 ms per token,  1524.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =  110603.71 ms /   213 tokens (  519.27 ms per token,     1.93 tokens per second)\n",
            "llama_print_timings:        eval time =    8912.44 ms /    13 runs   (  685.57 ms per token,     1.46 tokens per second)\n",
            "llama_print_timings:       total time =  119678.82 ms /   226 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    5947.07 ms\n",
            "llama_print_timings:      sample time =      54.65 ms /    82 runs   (    0.67 ms per token,  1500.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =  459169.22 ms /   904 tokens (  507.93 ms per token,     1.97 tokens per second)\n",
            "llama_print_timings:        eval time =   60655.47 ms /    81 runs   (  748.83 ms per token,     1.34 tokens per second)\n",
            "llama_print_timings:       total time =  520648.49 ms /   985 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I couldn't find the answer to the question \"What is a math teacher's favorite type of dessert?\" in the information provided. The context appears to be a collection of jokes and riddles, but there is no mention of a math teacher's favorite dessert. If you have any other questions or if there is anything else I can help you with, please let me know.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What is a math teacher’s favorite type of dessert?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjLKPHv3yFyC",
        "outputId": "46ba75f2-f37b-4914-edcc-4fd460948a73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    5947.07 ms\n",
            "llama_print_timings:      sample time =       3.41 ms /     5 runs   (    0.68 ms per token,  1465.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =  170714.97 ms /   306 tokens (  557.89 ms per token,     1.79 tokens per second)\n",
            "llama_print_timings:        eval time =    2663.28 ms /     4 runs   (  665.82 ms per token,     1.50 tokens per second)\n",
            "llama_print_timings:       total time =  173536.91 ms /   310 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    5947.07 ms\n",
            "llama_print_timings:      sample time =     228.15 ms /   324 runs   (    0.70 ms per token,  1420.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =  371421.99 ms /   736 tokens (  504.65 ms per token,     1.98 tokens per second)\n",
            "llama_print_timings:        eval time =  250836.90 ms /   323 runs   (  776.58 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:       total time =  624504.88 ms /  1059 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I couldn't find the answer in the information provided. However, I can provide you with some additional context that might help clarify the question.\n",
            "\n",
            "Humor is a complex and multifaceted concept that has been studied extensively by researchers in various fields such as psychology, anthropology, and neuroscience. While it is true that some primates produce laughter (Gervais & Wilson, 2005; Preuschoft & Hooff, 1997), humans are the only known species that use humor for making others laugh. This uniquely human trait has been present in every known human civilization throughout history (Caron, 2002; Gervais & Wilson, 2005).\n",
            "\n",
            "There have been several computational techniques developed to detect and generate humor, such as the HAHAcronym system (Stock, O., & Strapparava, C. (2005), Stock, O., & Strapparava, C. (2006)) and Suls' two-stage model for the appreciation of jokes and cartoons (Suls, J. M. (1972)). These systems have been used to improve our understanding of humor and its underlying mechanisms.\n",
            "\n",
            "In summary, while it is possible for machines to detect and generate humor, they do not possess the same level of complexity and nuance as human humor. Humor is a uniquely human trait that has evolved over time and continues to be an important aspect of human culture and communication.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What is humor?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4th3dTNJaYq0",
        "outputId": "d47af62c-a844-4f4f-b402-74bc452a9514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =     245.88 ms /   344 runs   (    0.71 ms per token,  1399.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =  377110.23 ms /   744 tokens (  506.87 ms per token,     1.97 tokens per second)\n",
            "llama_print_timings:        eval time =  303679.10 ms /   344 runs   (  882.79 ms per token,     1.13 tokens per second)\n",
            "llama_print_timings:       total time =  683125.02 ms /  1088 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Humor is a complex and multifaceted concept that has been studied extensively in psychology, anthropology, and other fields. While some primates produce laughter (Gervais & Wilson, 2005; Preuschoft & Hooff, 1997), humans are the only known species that use humor for making others laugh. This uniquely human trait has been a subject of interest for researchers who have developed various computational techniques to detect and generate humor.\n",
            "\n",
            "One such technique is the HAHAcronym system, which was developed by Stock and Strapparava (2005) to detect humor in text. The system uses a combination of linguistic and statistical analysis to identify patterns and structures that are associated with humor. It has been shown to be effective in detecting humor in various types of texts, including jokes, cartoons, and satirical articles.\n",
            "\n",
            "Another technique is the Laughing with hahacronym system, which was developed by Stock and Strapparava (2006) to generate humor. The system uses a combination of rule-based and machine learning algorithms to generate humorous text based on input prompts. It has been shown to be effective in generating humorous responses to various types of prompts, including questions, statements, and commands.\n",
            "\n",
            "In conclusion, while it may seem implausible for a machine to have any form of a sense of humor, these computational techniques demonstrate that it is possible to detect and generate humor using artificial intelligence. These systems can help us improve our understanding of this uniquely human trait and provide insights into the mechanisms underlying humor.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What is humor?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTyw8oSznsyE",
        "outputId": "aa753124-d5f7-4399-862d-c4666ae27454"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =       6.31 ms /     8 runs   (    0.79 ms per token,  1267.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =  200825.67 ms /   400 tokens (  502.06 ms per token,     1.99 tokens per second)\n",
            "llama_print_timings:        eval time =    6649.89 ms /     8 runs   (  831.24 ms per token,     1.20 tokens per second)\n",
            "llama_print_timings:       total time =  207692.51 ms /   408 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =      19.25 ms /    31 runs   (    0.62 ms per token,  1610.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =  741615.51 ms /  1380 tokens (  537.40 ms per token,     1.86 tokens per second)\n",
            "llama_print_timings:        eval time =   28059.57 ms /    30 runs   (  935.32 ms per token,     1.07 tokens per second)\n",
            "llama_print_timings:       total time =  770476.70 ms /  1410 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, I couldn't find the answer to your question. Can you please provide more context or clarify what you are asking?\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"make a joke\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaMznkmAnywi",
        "outputId": "5e037fc2-bf62-4203-ee48-51631e8c0978"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =       5.29 ms /     8 runs   (    0.66 ms per token,  1512.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =  247862.22 ms /   448 tokens (  553.26 ms per token,     1.81 tokens per second)\n",
            "llama_print_timings:        eval time =    5842.44 ms /     7 runs   (  834.63 ms per token,     1.20 tokens per second)\n",
            "llama_print_timings:       total time =  253942.39 ms /   455 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =      72.55 ms /   111 runs   (    0.65 ms per token,  1529.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =  429865.18 ms /   832 tokens (  516.66 ms per token,     1.94 tokens per second)\n",
            "llama_print_timings:        eval time =   92412.26 ms /   110 runs   (  840.11 ms per token,     1.19 tokens per second)\n",
            "llama_print_timings:       total time =  523230.32 ms /   942 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I understand the question is about a joke about cats. Based on the provided context, here are three jokes that involve cats:\n",
            "\n",
            "1. \"Why do cats hate computers?\" Because they can't get past the firewall!\n",
            "2. \"What do you call a cat with no teeth?\" A gummy worm.\n",
            "3. \"Why did the cat cross the road?\" To prove to the dog that he could do it!\n",
            "\n",
            "I hope these jokes bring a smile to your face and help you understand the humor behind them.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"I would like to hear a joke about cats\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lezQgNL20Lkb",
        "outputId": "7104a7ba-f609-4bce-ee44-b34043dffb63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =      13.30 ms /    10 runs   (    1.33 ms per token,   751.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =  306862.10 ms /   573 tokens (  535.54 ms per token,     1.87 tokens per second)\n",
            "llama_print_timings:        eval time =    7496.42 ms /     9 runs   (  832.94 ms per token,     1.20 tokens per second)\n",
            "llama_print_timings:       total time =  314685.00 ms /   582 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    6658.24 ms\n",
            "llama_print_timings:      sample time =      86.53 ms /   130 runs   (    0.67 ms per token,  1502.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =  432164.04 ms /   834 tokens (  518.18 ms per token,     1.93 tokens per second)\n",
            "llama_print_timings:        eval time =  114710.88 ms /   129 runs   (  889.23 ms per token,     1.12 tokens per second)\n",
            "llama_print_timings:       total time =  547997.16 ms /   963 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I understand the question is asking for three types of jokes about cats. Based on the provided context, here are three examples of cat-related jokes:\n",
            "\n",
            "1. \"Why do cats hate mice?\" - This joke plays off the fact that cats have a natural predator instinct towards mice.\n",
            "2. \"What did the cat say when it saw the mouse?\" - \"I'm on to you!\"\n",
            "3. \"Why don't cats play poker in the jungle?\" - Because there are too many cheetahs!\n",
            "\n",
            "These jokes are all related to cats and are meant to be humorous.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"please summarize the document jokes4\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xjK_8MyxET7"
      },
      "source": [
        "# Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNN5W0HrvtiL",
        "outputId": "6400505f-c66c-414e-b2e6-49cf47eed059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "+ off@0.0.10\n",
            "added 23 packages from 23 contributors and audited 23 packages in 1.846s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq streamlit --progress-bar off\n",
        "!npm install -qqq localtunnel --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obgfN_zrvuqJ",
        "outputId": "abb47b15-1f6b-4b09-9aef-e098c7b90627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.245.19.206\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.631s\n",
            "your url is: https://public-clocks-hide.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrTPHvEH3Kcf",
        "outputId": "ae32b493-2f8d-48f5-f959-0d10b1e8e830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting rag_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag_app.py\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content')\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "@st.cache_resource\n",
        "def init_llm():\n",
        "  return LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "                  max_tokens = 2000,\n",
        "                  temperature = 0.1,\n",
        "                  top_p = 1,\n",
        "                  n_gpu_layers = -1,\n",
        "                  n_ctx = 2048)\n",
        "llm = init_llm()\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "@st.cache_resource\n",
        "def init_memory(_llm):\n",
        "    return ConversationBufferMemory(\n",
        "        llm=llm,\n",
        "        output_key='answer',\n",
        "        memory_key='chat_history',\n",
        "        return_messages=True)\n",
        "memory = init_memory(llm)\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "  (4) Be funny.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"A not so funny machine\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Humorous minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"...i am so funny funny funny...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = chain(prompt)\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(answer[\"answer\"])\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ul10CfU5Jnt",
        "outputId": "e76b2036-c847-4277-af62-3d189e2e46de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.16.186.11\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.865s\n",
            "your url is: https://tricky-games-add.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run rag_app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
