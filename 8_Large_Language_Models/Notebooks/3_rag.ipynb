{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "Retrieval Augmented Generation (RAG) combines information retrieval with language models. It first searches for relevant facts in external sources, then feeds those facts to the language model alongside the user's prompt. This helps the model generate more accurate and factual responses, even on topics beyond its initial training data.\n"
      ],
      "metadata": {
        "id": "rQEWtbLY3-xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings 🛠️\n",
        "Let's get started by setting up a GPU on your Colab notebook. Head over to `Edit > Notebook Settings` and select `GPU` from the runtime type dropdown. Once you've made that change, click `Save`.\n",
        "\n",
        "Now, we'll need to install two additional libraries to build our RAG model. These libraries will help us create and store numerical representations of our text, which are essential for this task.\n",
        "\n",
        "1. **sentence_transformers:** This library will generate embeddings, which are like numerical summaries of our text. These embeddings will allow us to compare different sentences and identify relationships between them.\n",
        "\n",
        "2. **faiss-gpu:** This library provides a fast and efficient database for storing and retrieving our numerical summaries.\n",
        "\n",
        "> If you're using a CPU instead of a GPU, install `faiss-cpu` instead. This version will work just fine, but it may be a bit slower."
      ],
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ_oTkGwppsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966b3e2f-2ace-42bd-a790-383fe97f2eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM 🧠\n",
        "Here we'll add one extra parameter, `n_ctx`. This parameter controls the size of the input context. The larger the window, the more memory you're likely to use. The default of 512 is fine for most basic things, but as we are starting to retrieve articles and add them to the context window, we'll double the size to 1024. Feel free to play around with this number as your project needs."
      ],
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 1024)"
      ],
      "metadata": {
        "id": "-F76VzycqhLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.1.&nbsp;  Test your LLM"
      ],
      "metadata": {
        "id": "QWG2XPn15NgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_1 = llm.invoke(\"Write a poem about data science.\")\n",
        "print(answer_1)"
      ],
      "metadata": {
        "id": "fc8mzzu51d5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3.&nbsp; Retrieval Augmented Generation 🔃"
      ],
      "metadata": {
        "id": "yCif7c-y9hdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.&nbsp; Find your data\n",
        "Our model needs some information to work its magic! In this case, we'll be using a copy of Alice's Adventures in Wonderland, but feel free to swap it out for anything you like: legal documents, school textbooks, websites – the possibilities are endless!"
      ],
      "metadata": {
        "id": "SSdu2iln-Gi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/alice_in_wonderland.txt https://www.gutenberg.org/cache/epub/11/pg11.txt"
      ],
      "metadata": {
        "id": "Ghvsh8ag8-Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If your working locally, just download a txt book from Project Gutenburg. Here's the link to [Alice's Adventures in Wonderland](https://www.gutenberg.org/cache/epub/11/pg11.txt). Feel free to use any other book though."
      ],
      "metadata": {
        "id": "ZwoxAMLS9p_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.&nbsp; Load the data\n",
        "Now that we have the data, we have to load it in a format LangChain can understand. For this, Langchain has [loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/). There's loaders for CSV, text, PDF, and a host of other formats. You're not restricted to just text here.\n",
        "\n"
      ],
      "metadata": {
        "id": "0MKSMLO5-KN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"/content/alice_in_wonderland.txt\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "gYIfLLbIbkly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.&nbsp; Splitting the document\n",
        "Obviously, a whole book is a lot to digest. This is made easier by [splitting](https://python.langchain.com/docs/modules/data_connection/document_transformers/) the document into chunks. You can split it by paragraphs, sentences, or even individual words, depending on what you want to analyse. In Langchain, we have different tools like the RecursiveCharacterTextSplitter (say that five times fast!) that understand the structure of text and help you break it down into manageable chunks.\n",
        "\n",
        "Check out [this website](https://chunkviz.up.railway.app/) to help visualise the splitting process.\n"
      ],
      "metadata": {
        "id": "C3PsFsW6-L-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
        "                                               chunk_overlap=150)\n",
        "\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "FdwvsTy0bkjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.&nbsp; Creating vectors with embeddings\n",
        "\n",
        "[Embeddings](https://python.langchain.com/docs/integrations/text_embedding) are a fancy way of saying we turn words into numbers that computers can understand. Each word gets its own unique code, based on its meaning and relationship to other words. The list of numbers produced is known as a vector. Vectors allow us to compare text and find chunks that contain similar information.\n",
        "\n",
        "Different embedding models encode words and meanings in different ways, and finding the right one can be tricky. We're using open-source models from HuggingFace, who even have a handy [leaderboard of embeddings](https://huggingface.co/spaces/mteb/leaderboard) on their website. Just browse the options and see which one speaks your language (literally!).\n",
        "> As we are doing a retrieval project, click on the `Retrieval` tab of the leaderboard to see the best embeddings for retrieval tasks."
      ],
      "metadata": {
        "id": "LbNC8zhE-PcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)"
      ],
      "metadata": {
        "id": "2LlLwO788-Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The first time you use the embeddings model, it'll download all the necessary data. After that it runs locally on your machine. Unfortunately, Colab is a different story – its sessions end, so the model needs to download again each time.\n",
        "\n",
        "To exemplify using embeddings to transform a sentence into a vector, let's look at an example:"
      ],
      "metadata": {
        "id": "jBhmJNSMNPt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"Why do data scientists make great comedians? They're always trying to make ANOVA pun\"\n",
        "query_result = embeddings.embed_query(test_text)\n",
        "query_result"
      ],
      "metadata": {
        "id": "2SzwAVG30JfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "characters = len(test_text)\n",
        "dimensions = len(query_result)\n",
        "print(f\"The {characters} character sentence was transformed into a {dimensions} dimension vector\")"
      ],
      "metadata": {
        "id": "nB9NFaPg2fYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding vectors have a fixed length, meaning each vector produced by this specific embedding will always have 384 dimensions. Choosing the appropriate embedding size involves a trade-off between accuracy and computational efficiency. Larger embeddings capture more semantic information but require more memory and processing power. Start with the provided MiniLM embedding as a baseline and experiment with different sizes to find the optimal balance for your needs."
      ],
      "metadata": {
        "id": "12HrYsvv8-ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.&nbsp; Creating a vector database\n",
        "Imagine a library where books aren't just filed alphabetically, but also by their themes, characters, and emotions. That's the magic of vector databases: they unlock information beyond keywords, connecting ideas in unexpected ways."
      ],
      "metadata": {
        "id": "99m06CuX0CBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "XC74KL_Mbkga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the database is made, you can save it to use over and over again in the future."
      ],
      "metadata": {
        "id": "i95GEsABGfag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.save_local(\"/content/faiss_index\")"
      ],
      "metadata": {
        "id": "SSdCx7nCEZ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the code to load it again.\n",
        "\n",
        "We'll leave it commented out here as we don't need it right now - it's already stored above in the variable `vector_db`."
      ],
      "metadata": {
        "id": "GL9iRgt_GlQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_db = FAISS.load_local(\"/content/faiss_index\", embeddings)"
      ],
      "metadata": {
        "id": "g2HCPo-WEiiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also search your database to see which vectors are close to your input."
      ],
      "metadata": {
        "id": "o9C1-lCpGq4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.similarity_search(\"What does the Mad Hatter drink?\")"
      ],
      "metadata": {
        "id": "WzQuiYGQEzja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.&nbsp; Adding a prompt\n",
        "We can guide our model's behavior with a prompt, similar to how we gave instructions to the chatbot. We'll use specific tags in the prompt to tell the model what to do. These tags, `<s> </s>` and `[INST] [/INST]`, come straight from the [model's documentation on Hugging Face](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). They can also be found in [Mistral's docs](https://docs.mistral.ai/models/). Different models have different expectations for prompts, so always check the documentation."
      ],
      "metadata": {
        "id": "Ve31AV92-T_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "input_template = \"\"\"\n",
        "<s>\n",
        "[INST] Answer the question based only on the following context: [/INST]\n",
        "{context}\n",
        "</s>\n",
        "[INST] Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=input_template,\n",
        "                        input_variables=[\"context\", \"question\"])"
      ],
      "metadata": {
        "id": "J1LsKEpwbkXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7.&nbsp; RAG - chaining it all together\n",
        "This is the final piece of the puzzle, we now bring everything together in a chain. Our vector database, our prompt, and our LLM join to give us retrieval augmented generation."
      ],
      "metadata": {
        "id": "e86UMPFwE_rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_db.as_retriever(search_kwargs={\"k\": 2}), # top 2 results only, speed things up\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "id": "9JhOZwcl8-NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = qa_chain.invoke(\"Who likes to chop off heads?\")\n",
        "\n",
        "answer"
      ],
      "metadata": {
        "id": "cwiG3ZKkL1TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.7.1.&nbsp; Exploring the returned dictionary"
      ],
      "metadata": {
        "id": "rzbVPImoQKuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer.keys()"
      ],
      "metadata": {
        "id": "j5qPkIFpQS5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `query`\n",
        "\n",
        "The question that we asked."
      ],
      "metadata": {
        "id": "YDHJtk3SQEtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer['query']"
      ],
      "metadata": {
        "id": "sYG-m40m8-Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `result`\n",
        "\n",
        "The response."
      ],
      "metadata": {
        "id": "0--crcoQQG4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer['result']"
      ],
      "metadata": {
        "id": "eQQXlEfS89xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer['result'])"
      ],
      "metadata": {
        "id": "kEcB-6nk9Y47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### `source_documents`\n",
        "\n",
        "What information was used to form the response."
      ],
      "metadata": {
        "id": "KhxTs6IKQJGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer['source_documents']"
      ],
      "metadata": {
        "id": "pmYjIGvc8_XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer['source_documents'][0]"
      ],
      "metadata": {
        "id": "0R2VVZAE9B6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer['source_documents'][0].page_content"
      ],
      "metadata": {
        "id": "xa8eSQAz9DrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer['source_documents'][0].page_content)"
      ],
      "metadata": {
        "id": "mbw38Al09FlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The documents name also gets returned, useful if you have multiple documents!"
      ],
      "metadata": {
        "id": "-Uzcci0pHI43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer['source_documents'][0].metadata"
      ],
      "metadata": {
        "id": "mzqDKDaAAA2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer['source_documents'][0].metadata[\"source\"]"
      ],
      "metadata": {
        "id": "8cK_y5OcAKu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Challenge 😀\n",
        "Build a RAG chain that you can use to question the PDF of the python version of `An Introduction to Statistical Learning`. This is an amazing book covering all the maths behind machine learning, you should definitely keep a copy on your hard drive.\n",
        "* https://www.statlearning.com/\n",
        "* https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf"
      ],
      "metadata": {
        "id": "Kyk600ivwAxd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2G-9qcgF0_sZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}